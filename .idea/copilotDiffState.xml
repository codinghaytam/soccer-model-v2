<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/models/models.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/models/models.py" />
              <option name="originalContent" value="import torch&#10;import torch.nn as nn&#10;&#10;&#10;class LSTMScorer(nn.Module):&#10;    def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 2):&#10;        super().__init__()&#10;        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)&#10;        self.fc = nn.Linear(hidden_dim, 1)  # regression score&#10;&#10;    def forward(self, x, lengths=None):&#10;        # x: (B, T, F). lengths optional for variable length (not strictly required here)&#10;        packed_out, (hn, _) = self.lstm(x)&#10;        out = self.fc(hn[-1])  # last layer hidden state&#10;        return out.squeeze(-1)&#10;&#10;&#10;class CNN1DScorer(nn.Module):&#10;    def __init__(self, n_joints: int = 17, channels: int = 2):&#10;        super().__init__()&#10;        in_ch = n_joints * channels&#10;        self.conv = nn.Sequential(&#10;            nn.Conv1d(in_ch, 64, kernel_size=3, padding=1),&#10;            nn.ReLU(),&#10;            nn.Conv1d(64, 128, kernel_size=3, padding=1),&#10;            nn.ReLU(),&#10;            nn.AdaptiveAvgPool1d(1),&#10;        )&#10;        self.fc = nn.Linear(128, 1)&#10;&#10;    def forward(self, x, lengths=None):&#10;        # x: (B, T, F) with F=n_joints*channels&#10;        x = x.transpose(1, 2)  # -&gt; (B, F, T)&#10;        x = self.conv(x).squeeze(-1)&#10;        out = self.fc(x)&#10;        return out.squeeze(-1)&#10;&#10;&#10;" />
              <option name="updatedContent" value="import torch&#10;import torch.nn as nn&#10;&#10;&#10;class LSTMScorer(nn.Module):&#10;    def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 2):&#10;        super().__init__()&#10;        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)&#10;        self.fc = nn.Linear(hidden_dim, 1)  # regression score&#10;&#10;    def forward(self, x, lengths=None):&#10;        # x: (B, T, F). lengths optional for variable length (not strictly required here)&#10;        packed_out, (hn, _) = self.lstm(x)&#10;        out = self.fc(hn[-1])  # last layer hidden state&#10;        return out.squeeze(-1)&#10;&#10;&#10;class CNN1DScorer(nn.Module):&#10;    def __init__(self, input_dim: int):&#10;        super().__init__()&#10;        self.conv = nn.Sequential(&#10;            nn.Conv1d(input_dim, 64, kernel_size=3, padding=1),&#10;            nn.ReLU(),&#10;            nn.Conv1d(64, 128, kernel_size=3, padding=1),&#10;            nn.ReLU(),&#10;            nn.AdaptiveAvgPool1d(1),&#10;        )&#10;        self.fc = nn.Linear(128, 1)&#10;&#10;    def forward(self, x, lengths=None):&#10;        # x: (B, T, F) where F=input_dim&#10;        x = x.transpose(1, 2)  # -&gt; (B, F, T)&#10;        x = self.conv(x).squeeze(-1)&#10;        out = self.fc(x)&#10;        return out.squeeze(-1)&#10;&#10;&#10;class LSTMClassifier(nn.Module):&#10;    def __init__(self, input_dim: int, hidden_dim: int = 64, num_layers: int = 2, num_classes: int = 2):&#10;        super().__init__()&#10;        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)&#10;        self.fc = nn.Linear(hidden_dim, num_classes)&#10;&#10;    def forward(self, x, lengths=None):&#10;        # x: (B, T, F)&#10;        _, (hn, _) = self.lstm(x)&#10;        out = self.fc(hn[-1])  # (B, num_classes)&#10;        return out&#10;&#10;&#10;class CNN1DClassifier(nn.Module):&#10;    def __init__(self, input_dim: int, num_classes: int):&#10;        super().__init__()&#10;        self.conv = nn.Sequential(&#10;            nn.Conv1d(input_dim, 64, kernel_size=3, padding=1),&#10;            nn.ReLU(),&#10;            nn.Conv1d(64, 128, kernel_size=3, padding=1),&#10;            nn.ReLU(),&#10;            nn.AdaptiveAvgPool1d(1),&#10;        )&#10;        self.fc = nn.Linear(128, num_classes)&#10;&#10;    def forward(self, x, lengths=None):&#10;        # x: (B, T, F) where F=input_dim&#10;        x = x.transpose(1, 2)  # -&gt; (B, F, T)&#10;        x = self.conv(x).squeeze(-1)&#10;        logits = self.fc(x)&#10;        return logits" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>